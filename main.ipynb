{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d87f75b5-5597-465b-b0c7-f8a70909852a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded successfully.\n",
      "Train features shape: (75000, 1666)\n",
      "Test features shape: (75000, 1666)\n",
      "\n",
      "Applied log transform (log1p) to the price data.\n",
      "\n",
      "Training the regression model on log-transformed prices...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.872724 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 424575\n",
      "[LightGBM] [Info] Number of data points in the train set: 75000, number of used features: 1665\n",
      "[LightGBM] [Info] Start training from score 2.708050\n",
      "Model training complete.\n",
      "\n",
      "Making predictions on the test data...\n",
      "Converted predictions back to the original price scale.\n",
      "\n",
      "Creating the submission file...\n",
      "\n",
      "Submission file 'submission.csv' created successfully!\n",
      "Submission file head:\n",
      "   sample_id      price\n",
      "0     100179  17.678548\n",
      "1     245611  20.294969\n",
      "2     146263  22.746099\n",
      "3      95658   7.932223\n",
      "4      36806  18.751184\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Load All Necessary Data ---\n",
    "print(\"Loading data...\")\n",
    "\n",
    "# Load the features you created\n",
    "X_train = pd.read_csv(\"X_train_features.csv\")\n",
    "X_test = pd.read_csv(\"X_test_features.csv\")\n",
    "\n",
    "# Load original data to get the target variable ('price') and sample IDs\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    y_train = train_df['price']\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Make sure original train.csv and test.csv are available.\")\n",
    "except KeyError:\n",
    "    print(\"Error: The column 'price' was not found in train.csv.\")\n",
    "\n",
    "print(\"Data loaded successfully.\")\n",
    "print(f\"Train features shape: {X_train.shape}\")\n",
    "print(f\"Test features shape: {X_test.shape}\")\n",
    "\n",
    "# --- 2. CRITICAL CHANGE: Apply Log Transform to the Target Variable ---\n",
    "# This helps the model handle the skewed distribution of price data and\n",
    "# significantly improves SMAPE score.\n",
    "y_train_log = np.log1p(y_train)\n",
    "print(\"\\nApplied log transform (log1p) to the price data.\")\n",
    "\n",
    "\n",
    "# --- 3. Train the LightGBM Regression Model ---\n",
    "# Using more robust hyperparameters\n",
    "model = lgb.LGBMRegressor(\n",
    "    objective='regression_l1', # MAE is a good objective for price data\n",
    "    metric='rmse',             # Use RMSE for training, as it's sensitive to large errors\n",
    "    random_state=42,\n",
    "    n_estimators=2000,         # Increased estimators\n",
    "    learning_rate=0.01,        # Reduced learning rate\n",
    "    num_leaves=40,\n",
    "    max_depth=7,\n",
    "    colsample_bytree=0.7,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining the regression model on log-transformed prices...\")\n",
    "# Train on the LOG-TRANSFORMED target\n",
    "model.fit(X_train, y_train_log)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "\n",
    "# --- 4. Make Predictions and Inverse Transform ---\n",
    "print(\"\\nMaking predictions on the test data...\")\n",
    "# Predict on the test set, which will give log-scale predictions\n",
    "predictions_log = model.predict(X_test)\n",
    "\n",
    "# CRITICAL CHANGE: Convert predictions back to the original price scale\n",
    "# np.expm1 is the inverse of np.log1p\n",
    "predictions = np.expm1(predictions_log)\n",
    "\n",
    "# CONSTRAINT: Ensure all predicted prices are positive\n",
    "predictions[predictions < 0] = 0\n",
    "print(\"Converted predictions back to the original price scale.\")\n",
    "\n",
    "\n",
    "# --- 5. Create the Submission File ---\n",
    "print(\"\\nCreating the submission file...\")\n",
    "submission_df = pd.DataFrame({\n",
    "    'sample_id': test_df['sample_id'],\n",
    "    'price': predictions\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission2.csv', index=False, float_format='%.4f')\n",
    "\n",
    "print(\"\\nSubmission file 'submission.csv' created successfully!\")\n",
    "print(\"Submission file head:\")\n",
    "print(submission_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14160ff8-15ce-402e-a992-e7b390665f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features and labels...\n",
      "Training data shape: (60000, 1666)\n",
      "Validation data shape: (15000, 1666)\n",
      "\n",
      "Training model on the 80% split...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.936320 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 424575\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 1665\n",
      "[LightGBM] [Info] Start training from score 14.090000\n",
      "Making predictions on the 20% validation set...\n",
      "\n",
      "-------------------------------------\n",
      "Validation SMAPE Score: 59.8039%\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 1. Define the SMAPE Metric Function ---\n",
    "def smape(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the Symmetric Mean Absolute Percentage Error (SMAPE).\n",
    "    \"\"\"\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    # Handle the case where both true and predicted are zero\n",
    "    # to avoid division by zero\n",
    "    ratio = np.where(denominator == 0, 0, numerator / denominator)\n",
    "    return np.mean(ratio) * 100\n",
    "\n",
    "# --- 2. Load Your Data ---\n",
    "print(\"Loading features and labels...\")\n",
    "X = pd.read_csv(\"X_train_features.csv\")\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "y = train_df['price']\n",
    "\n",
    "# --- 3. Split Data into Training and Validation Sets ---\n",
    "# Using 80% for training and 20% for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Validation data shape: {X_val.shape}\")\n",
    "\n",
    "# --- 4. Train Model on the SPLIT Data ---\n",
    "print(\"\\nTraining model on the 80% split...\")\n",
    "model = lgb.LGBMRegressor(\n",
    "    objective='regression_l1',\n",
    "    random_state=42,\n",
    "    n_estimators=1000\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# --- 5. Predict on Validation Set and Calculate Score ---\n",
    "print(\"Making predictions on the 20% validation set...\")\n",
    "val_predictions = model.predict(X_val)\n",
    "\n",
    "# Ensure predictions are non-negative\n",
    "val_predictions[val_predictions < 0] = 0\n",
    "\n",
    "# Calculate and print the score\n",
    "validation_smape = smape(y_val, val_predictions)\n",
    "\n",
    "print(\"\\n-------------------------------------\")\n",
    "print(f\"Validation SMAPE Score: {validation_smape:.4f}%\")\n",
    "print(\"-------------------------------------\")\n",
    "# Lower SMAPE is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85de33bc-c40d-4733-bea7-36601cca3ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
